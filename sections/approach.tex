% !TeX root = ../paper.tex
% !TeX encoding = UTF-8
% !TeX spellcheck = en_US

\section{Distributed discovery of \glspl{od}}\label{sec:approach}

  We now present our approach to \gls{od} discovery, called \dodo{}.
  First, we explain how \ocddiscover{}~\cite{consonni} traverses the candidate space to find minimal \glspl{ocd} and \glspl{od} in \cref{sec:ocddiscover}.
  This is the algorithmic foundation of our approach, which we slightly adopt to be able to distribute the discovery across several nodes in a cluster (see \cref{sec:dodo}).
  \Cref{sec:architecture} then describes \dodo{}'s architecture and \cref{sec:protocols} the communication protocols used to make a \dodo{} cluster fault-tolerant and elastic.
 
\subsection{The \ocddiscover{} algorithm}\label{sec:ocddiscover}

  The \ocddiscover{} algorithm creates the search space to find \glspl{od} over \glspl{ocd}~\cite{consonni}.
  This reduces the number of candidates that the algorithm must check, because \glspl{ocd} are symmetrical.
  \Citeauthor{consonni} prove that if a \gls{od} holds, than a functional dependency and a \gls{ocd} hold as well.
  Based on this theorem, they then use a breadth-first search strategy to identify \gls{ocd} relations in the dataset to discover minimal dependencies before longer ones.

  The \ocddiscover{} algorithm consists of two phases.
  In the first phase, \ocddiscover{} performs an initial pruning step that removes constant columns and reduces order equivalent columns of the original column set.
  The constant columns and order equivalent columns are part of the output.
  They would generate a huge amount of \glspl{od} and those \glspl{od} can easily reconstructed from the result later on.

  In the second phase, the actual \gls{od} discovery takes place.
  \ocddiscover{} generates \gls{ocd} candidates from the remaining columns in levels, one after the other.
  For each level the algorithm checks if the \gls{ocd} candidates hold.
  If a candidate holds, the candidate is further checked for being order dependent, the results are emitted, and child \gls{ocd} candidates are generated.
  This step includes further pruning rules~\cite{consonni}.
  If the \gls{ocd} candidate does not hold, no new candidates starting from it are generated.
  All newly generated candidates are then put into a list for the next level.

  \Citeauthor{consonni} proved in their paper, that they would be able to find all minimal \glspl{od}.
  However, as \citeauthor{szlichta:errata} showed in their Errata Note, \citeauthor{consonni} made a mistake in one of their proofs and this method of building the search space does not create minimal \glspl{od} with repeated prefixes~\cite{szlichta:errata}.

\subsection{The \dodo{} algorithm}\label{sec:dodo}

  We use \citeauthor{consonni}'s work as the basis of our discovery algorithm.
  Conceptually, \dodo{} works the same as \ocddiscover{}.
  This means that we also make a breadth-first search in a \gls{ocd} candidate tree and our algorithm also consists of an initial pruning step followed by the actual search step.
  However, our implementation of the algorithm differs.

  Instead of generating our candidates level-wise, we use a task-based approach with a single task queue.
  We initiate the task queue with the initial \gls{ocd} candidates comparable to the first level generation by \citeauthor{consonni}.
  Each task's input is the \gls{ocd} candidate.
  Its outputs are the results from checking the candidate and a set of new tasks that can be empty.
  The results are emitted as output and the new tasks (child candidates) are added to the task queue.
  This is possible, because \gls{ocd} candidate checking and the generation of new candidates is independent from the other candidates~\cite{consonni}.
  This setup allows us to parallelize the processing, because the tasks in the task queue can be processed independently and concurrently.
  To distribute our algorithm, we just spread the tasks, ideally evenly, across our cluster.

\subsection{Implementation of the \dodo{} algorithm}\label{sec:architecture}

  We implement the \dodo{} algorithm using Akka~\cite{akka}, Akka Clustering~\cite{akka:clustering} and the Scala programming language~\cite{scala}.
  Akka is a toolkit for building distributed message-driven applications using the actor programming model that was first introduced in Orleans~\cite{bernstein:orleans}.
  It provides tools for concurrency, distribution, and elasticity.

  The \dodo{} algorithm is designed as a peer-to-peer cluster.
  All nodes in the cluster are equal and employ the same internal architecture.
  The nodes are arranged on a ring, so that every node has two neighbors.
  Based on this cluster layout, we define communication protocols (see \cref{sec:protocols}) to balance the workload, to recover from node failures, and to allow dynamic cluster sizes.
  Each node runs its own actor system with the complete set of \dodo{} actors (\cref{sec:node-architecture} describes them in more detail) and works on the checking of \glspl{ocd} and the generation of new \gls{ocd} candidates.
  \Cref{fig:dodo-architecture} shows the cluster architecture and the internal architecture of the nodes' actor systems exemplarily by node two.

  \begin{figure}
    \centering
    \input{pictures/tikz/dodo-architecture}
    \caption{DODO architecture}
    \label{fig:dodo-architecture}
  \end{figure}

  The execution of the \dodo{} algorithm in the cluster is initiated by a seed node.
  The seed node plays a special role in our cluster and it is the only exception to the peer-to-peer approach.
  It is started before all other nodes and has the following unique tasks:

  \begin{itemize}
    \item The seed node is the initial contact point for all nodes that want to join the cluster.
    \item It always loads the dataset from disk into memory.
    \item It performs the initial pruning and generates the first \gls{ocd} candidates.
  \end{itemize}

  The seed node is the first node in the cluster and may be the only one in the cluster for some time.
  If the seed node fails during this first phase, the whole cluster shuts down and must be restarted.
  We do not implement resilience for this first phase.
  This seemed to be a reasonable constraint, though, because the first phase only takes a few seconds.

  After the seed node started up, it immediately starts with the discovery phase.
  All other nodes that already joined the cluster wait for the seed node to finish data loading and initial pruning.
  The discovery phase can not start until the seed node removed all constant columns and reduced the order equivalent columns.
  Therefore, the seed node broadcasts the reduced column set to all joined nodes once it finished pruning them.
  Our algorithm requires all nodes to have access to the whole dataset, because we do not partition the candidates and every node can check every candidate.
  This means that nodes load the dataset from their neighbors using data streaming, where the seed node is the data origin.

  Every node needs the dataset, the reduced column set, and some \gls{ocd} candidates to start the discovery.
  Later joining nodes, therefore, ask their direct neighbors in the ring for the dataset and the reduced column set.
  The distribution of \gls{ocd} candidates is implemented using a work stealing protocol, which we describe in \cref{protocol:workStealing}.

\subsubsection{Node Architecture}\label{sec:node-architecture}
Every node in the cluster shares the same actors.
These are both helpful for parallelizing the tasks each node work on as well as structuring the communication in between nodes.

\begin{description}
  \item[Master]
  The \emph{Master actor} is responsible for holding the state of a node.
  In contains the \emph{Candidate queue} and \emph{Pending queue} and distributes the candidates to the \emph{Worker actors} to be worked on in parallel. 
  We use a pull-based approach for this distribution of tasks.
  This means that the \emph{Master} only ever responds to \emph{Workers} asking for candidates and therefore does not have to monitor the state of every single \emph{Worker} of the node.
  The \emph{Master} also deals with getting new candidates using \emph{Work Stealing} (\cref{protocol:workStealing}), when the \emph{Candidate Queue} is empty and initiates the node shutdown when the \emph{Downing Protocol} (\cref{protocol:downing}) reveals that all \glspl{od} have been found.
  
  \item[Worker]
  A node can have an arbitrary number of \emph{Worker actos}, though the number of available threads - 1 is usually a good estimation for effective resources usage.
  The \emph{Workers} check candidates, which they get from the \emph{Master} and generate new ones, which they send back for the \emph{Master} to append to the \emph{Candidate Queue}.
  This checking and generating of candidates is the main task of each node and therefore requires the most resources.
  All \glspl{od} and \glspl{ocd} the \emph{Worker} finds are send to the \emph{Result Collector} to be recorded into a file. 
  
  \item[Data Holder]
  The \emph{Data Holder actor} is responsible for loading and holding the dataset, for which the \glspl{od} are to be found. 
  A reference to this dataset is shared with the \emph{Workers} who need it to check candidates.
  If a path to the dataset is provided to the node on start up, it loads it from the file.
  If not, the \emph{Data Holder} asks one of their neighbors in the cluster to provide them the dataset via streaming.
  Once the dataset is loaded, the \emph{Data Holder} notifies the \emph{Master} that the node is ready to start working on checking candidates. 
  
  \item[Result Collector]
  The \emph{Result Collector actor} gets send all found \glspl{od} and \glspl{ocd} by the \emph{Workers}.
  It collects and counts these results and writes them to a file so they can be recovered even if the node fails unexpectedly. 
  
  \item[Cluster Listener]
  The \emph{Cluster Listener actor}'s task is to determine and share who the neighbors of its node are.
  Whenever a new node joins the cluster all previous send it references to their \emph{Master} and \emph{State Replicator actors}.
  The new \emph{Cluster Listener} sends back the corresponding information for its node. \\
  When the ordering of nodes changes because a node joined or left the cluster, the updated neighbors are send to the \emph{State Replicator}.
  In contrast, the \emph{Data Holder} only receives information about the neighboring nodes, when it specifically asks for them, because it only needs the information in the beginning to know whom to ask for the dataset.
  
  \item[State Replicator]
  The \emph{State Replicator} holds the state of its neighboring nodes and shares the \emph{Master's} state with them.
  This enables us to recover work from unexpectedly failed nodes and is explained in more detail in the \emph{State Replicaton Protocol} (see \cref{protocol:stateReplication}).
  
  \item[Reaper] 
  The \emph{Reaper Actor} watches all other actors and cleanly shuts down the system once all of them are terminated.
\end{description}

\subsection{Communication Protocols}\label{sec:protocols}
% Questions to be answered for each one:
% What is its purpose? 
% When is the protocol started and what is the desired outcome? 
% Who is communicating? 
% What Messages are being used?
% What cases need to be covered? (Early outs etc)

\subsubsection{Work Stealing Protocol}\label{protocol:workStealing}
% Possible picture (sequence diagram): https://sequencediagram.org/index.html#initialData=C4S2BsFMAIHUHsBOBraBlYkCG4QDsBzaABUXmHgGN5wAoWgFQAsRIAzaPeAE0gFoAfF17QATAC4A4pGAIU4eFm6MW7Tj35CN0AMxSZc5AqUrWHYZovQALPtlIji5VdGDmZ9b3GHj3ABQAjAAMAJS0VjpuquYa3g6+fgBsYVbWUR4WcfJOfgAcYe5qFulFsQwgALaQ8ACuwKalvIIu4hjY4IZ+1gXRnpbatm04naIp2q4ChTFeGDSQeJ3W0JRYeNwg3FiYAM5jImmTvZmzUAsOfqLLq+ubOz0ZGs3j4gCClMgn84YN0-37r+9PmcUEA
When a node is out of work, either because it just joined the cluster or because all the candidates it checked got pruned and did not generate any new candidates to check, it tries to take over some of the work of the other nodes. 
This process is called \emph{Work Stealing}.
We use it to ensure that no node is idle while the others are still checking candidates and to balance the workload over the cluster. 
The workload of a node is defined by the number of candidates waiting to be processed in its queue. 
In the desired outcome to add to its own queue of the \emph{Work Stealing Protocol}, each node ends up with a similar workload. 
To achieve this goal the \emph{Master Actors} of the different nodes communicate their workloads with each other.\\
The \emph{Work Stealing Protocol} is initiated when a node's \emph{Candidate Queue} is empty, even if some of its workers are still processing candidates and might return new ones soon.
This node (the \emph{Thief node}) sends a message to all other nodes, asking for the current size of their \emph{Candidate Queues}.
It also sets a timeout after which it processes the answers of the other nodes. 
During this processing step it calculates the average \emph{Candidate Queue's} length of all the nodes that answered and itself.
To improve the balancing of workloads over the cluster, the \emph{Thief node} only takes candidates from nodes that have a \emph{Candidate Queue} of above average length and only takes so many candidates that its own \emph{Candidate Queue's} length does not exceed the average. 
% Explain in more detail / with pseudocode?
To get the candidates from another node's \emph{Candidate Queue}, the \emph{Thief node} sends a message with the amount of candidates it wants to take to the node it wants to take them from.
This node, the \emph{Victim node}, then moves the asked for candidates from its \emph{Candidate Queue} into its \emph{Pending Queue} and sends them to the \emph{Thief node}.
However, since the \emph{Victim node's} queue might have changed since it first sent its length to the \emph{Thief node}, the \emph{Victim node} sends back at most half of the candidates in its queue.
Once the \emph{Thief node} receives the candidates, it adds them to its own queue and sends a message acknowledging the receipt to the \emph{Victim node}, which then deletes the candidates from its \emph{Pending queue}. 
To ensure the send candidates arrive and will get processed, the \emph{Victim node} watches the \emph{Thief node}.
In the case that the \emph{Thief node} fails before it could acknowledge the receipt of the candidates, the \emph{Victim node} moves them back to its own \emph{Candidate queue} from its \emph{Pending queue}.
 
\subsubsection{Downing Protocol}\label{protocol:downing}
When all \glspl{od} have been found and there are no more candidates to check, the system should shut itself down.
A node starts the \emph{Downing Protocol}, when it has no more candidates in its \emph{Candidate Queue} and \emph{Pending Queue} and an attempt to get more work using \emph{Work Stealing} was unsuccessful.
Checking the \emph{Pending Queue} is important because candidates currently being processed could generate new candidates and therefore new work.\\
To ensure that there are no more candidates to be processed and no more candidates being generated in the whole cluster, the node has to ask every other node, whether they are also out of candidates.
If another node still holds unprocessed candidates or is still in the process of creating new candidates, these could be redistributed over the cluster using \emph{Work Stealing} so the node can continue to work instead of shutting itself down.
The node waits for a response from every other node in the cluster, also being aware of nodes leaving the cluster and subsequently not awaiting their response anymore.\\
If any of the other nodes respond with a message saying that they are still holding or processing candidates, the node switches from the \emph{Downing} to the \emph{Work Stealing Protocol}.
If all nodes have either left the cluster or responded that they have no more work, the node shuts itself down.

\subsubsection{State Replication Protocol}\label{protocol:stateReplication}
To ensure any of the nodes of the cluster can fail without any work getting lost, every node regularly replicates its state to two other nodes.
Because of this, three specific nodes would have to fail simultaneously to permanently lose any unprocessed candidates. 
In this case, everything would need to be restarted to ensure complete results. \\
Which nodes the state is sent to is decided by their addresses.
All nodes in a cluster are ordered by their addresses and every node replicates its state to both its neighbors in this ordering.
The first and last node in this ordering are also each others neighbors.\\
Every node has a \emph{State Replication Actor} responsible for regularly sending the node's state to its neighbors and processing these neighbor's states when they are sent.
It gets the information of who its neighbors are from the \emph{Cluster Listener Actor}. \\
In regular intervals, the \emph{State Replicator} asks the \emph{Master Actor} for its current state.
The state is a combination of its \emph{Candidate Queue} and \emph{Pending Queue}.
On receiving the \emph{Master's} current state, the \emph{State Replicator} sends to both its neighbors with a version number that is continually increased.\\
When a \emph{State Replicator} receives the state of one of its neighbors, it checks whether the version number in the new message is higher than the version number of the last state it received from that actor.
If this is the case, it saves the new state and version number.
If not, is discards the new message, because it already has a more up to date state for that actor.

\paragraph{A node joins the cluster}
When a new node joins the cluster, the \emph{Cluster Listener} updates its ordering of all the nodes in the cluster. 
It then sends the \emph{State Replicator} the updated neighbor information.
If the nodes neighbor has changed, the \emph{State Replicator} removes the state of the node that is no longer its neighbor and sends its own state to its new neighbor.

\paragraph{A node leaves the cluster}
When a node leaves the cluster, its two neighbors have to decide which of them will take on the work the leaving node did not finish.
They are alerted to their neighbor's leaving by the \emph{Cluster Listener} who also immediately tells them, who their new neighbor is.
This new neighbor was the leaving nodes other neighbor.
To determine which of the two has the most up to date state of the leaving node, they share the version number of the leaving node's state. 
The node with the higher version number adds the leaving nodes state to its own \emph{Candidate Queue} to ensure all candidates get processed. 
The node with the lower version number deletes the state of the leaving node.
The two nodes then share their state with each other. 